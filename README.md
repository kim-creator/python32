StandardScaler

평균을 0으로, 분산을 1로 변경해서 모든 특성이 같은 크기를 가지게 한다.

최솟값과 최댓값의 크기를 제한하지는 않는다.

x−x¯σ

x : 특성의 값

x¯ : x의 평균

σ : 표준편차

이 결과를 z-score 라고도 한다.

RobustScaler

StandardScaler와 비슷하게 특성들이 같은 스케일을 갖게 된다.

평균과 분산 대신 중앙값(median, q2)과 사분위 값을 사용한다.

중간값(median)이란?

x보다 작은 수가 절반이고, x보다 큰 수가 절반인 x

x−q2q3−q1

q1 : x보다 작은 수가 전체 개수의 14 - 25%

q3 : x보다 큰 수가 전체 개수의 14 - 75%

MinMaxScaler

모든 특성이 정확하게 0 ~ 1사이의 위치하도록 데이터를 변형

2차원 데이터 세트라면? x축의 0 ~ 1, y축의 0 ~ 1 사각형 모양안에 데이터 포인트가 배치

x−xminxmax−xmin

Normalizer

특성 벡터의 유클라디안 거리가 1이 되도록 데이터 포인트를 배치

원점을 중심으로 하는 반지름의 길이가 1인 원의 호 위에 데이터포인트를 배치

StandardScaler, MinMaxScaler, RobustScaler와 매우 다른 정규화 기법

벡터의 크기 (데이터의 크기)와는 상관 없고 데이터의 방향( 또는 각도 )이 중요할 때 사용

Xtest−Xtrain−minXtrain−max−Xtrain−min

훈련 데이터와 테스트 데이터의 스케일을 같은 방법으로 조정하면??

훈련 세트를 이용해서 스케일러를 만들고, 테스트 세트를 이용해서 새로운 스케일러를 만들면?

훈련 세트를 이용해서 만든 스케일러를 테스트 세트에도 적용시키면?

항상 스케일러는 TRAIN DATA SET로만 만들자! TEST DATA SET 으로 만들면 원본 데이터의 분포가 깨져버린다.

연속형 특성과 범주형 특성

연속형 특성(continuous feature) : 실수 형태로 측정된 데이터

범주형 특성(categorical feature) : 다른 말로 이산형 특성(discreate feature) 라고도 함. 범주(카테고리)를 의미하는 데이터

카테고리를 의미하는 데이터 이기 때문에 보통 텍스트나 정수로 이루어져 있다.

특성종류

특성형태

연속형 특성 예시

5.11123

범주형 특성 예시

청바지

특성 공학을 이용한 데이터 표현의 중요성

데이터가 어떤 형태의 특성으로 구성되어 있는가 보다( 연속형인지, 범주형인지 ) 데이터를 어떻게 표현하는지가 머신러닝 성능에 영향을 많이 준다.

스케일링 작업 같은 경우도 데이터를 어떻게 표현하는지에 대한 방법

특성간의 상호작용(특성간의 곱)이나 다항식을 추가 특성으로 넣는 것이 도움이 될때가 많다.

어떠한 어플리케이션에 적합한 데이터 표현을 찾는 것을 특성 공학이라고 한다.

기본적으로, 머신러닝 훈련에는 텍스트데이터는 들어갈 수가 없다.

원-핫 인코딩 ( 가변수 )

범주형 변수를 표현하는데 가장 널리 쓰이는 방법

원-아웃-오브 N 인코딩

가변수 dummy variable 이라고도 한다.

label로 생각해야할 데이터가 문자열 범주형 형태의 데이터라면 OrdinalEncoder를 사용하자.

OneHotEncoding은 feature에만 사용하자.

구간분할

데이터에 구간을 만들어서 구간별 예측을 수행.

원본 데이터에 구간정보를 부여해 준다.

구간별 예측을 하기 위해 구간정보를 만들어 준다.

구간 데이터는 정수형태의 데이터 지만, 범주형 변수의 성질을 띄고있음!

One Hot Encoding을 해 준다.

상호작용과 다항식

상호작용 ( interaction ) : 데이터들 끼리의 관계를 설정하는 것. ( 보통은 곱이나, 연관된 특성을 넣어주는 것 )

구간데이터(X_binned)와 실제 그 구간에 있었던 데이터(X)

X축 특성이 하나이기 때문에 기울기도 하나다.

구간 데이터와 더불어 구간 데이터의 X 값이 같이 있으면? - X가 속한 구간 데이터와 실제 그 구간에 속한 X의 값을 동시에 볼 수 있다.

원본 특성의 다항식 추가

구간 분할은 연속형 특성을 확장하는 하나의 방법.

원본 특성(x)의 다항식을 추가하는 방법

어떠한 특성 x에 대해 x2, x3, x4, ⋯ 등등을 새롭게 추가하는 방식

사이킷 런의 preprocessing 모듈의 PolynomialFeatures에 구현

10차원 ( degree=10 )으로 변환 했기 때문에 9개의 특성이 새롭게 추가 된다. (즉 원본 1개 + 새로운거 9개 = 10개)

include_bias=True로 했으면 편향이 하나 더 붙으니까 11개

특성 선택

여러 변환 과정을 거치다 보면 필요 이상으로 필요 없는 특성들이 추가적으로 생길 수도 있다.

원본 데이터 세트를 가지고 왔는데, 노이즈 데이터가 많을 수도 있다.

비즈니스 전문가가 없다면? 엔지니어가 직접 데이터를 정제해야 하는 일들이 생김

어떠한 데이터가 좋은 데이터이고, 어떠한 데이터가 나쁜 데이터인가?

특성 자동 선택

일변량 통계 : Unvariable statistics

모델 기반 선택 : model-based Selection

반복적 선택 : Iterative Selection

일변량 통계

각각의 특성과 타깃 사이에 중요한 통계적 관계가 있는지를 계산한다.

깊게 연관되어 있는 특성을 선택한다.

분류에서는 분산 분석이라고 한다.

각 특성이 독립적으로 평가 - 일변량

다른 특성과 깊게 연관된 특성은 선택이 안될 가능이 크다.

지지도 : 타깃이 특성을 얼마나 지지하는가에 대한 점수(p-값)

SelectKBest - 지지도가 좋은 K개의 특성만 사용하기

SelectPercentile - 지정된 비율만큼 특성을 선택

get_support() 함수를 활용하여 어떤 특성이 선택 되었는지 알 수 있다.

모델 기반 특성 선택

지도학습 머신러닝 모델을 이용해서 특성의 중요도를 평가해서 가장 중요한 특성만 선택

특성 선택을 위해 사용하는 머신러닝 모델을 최종적으로 사용할 지도학습 모델과 같을 필요는 없다.

결정 트리 같은 트리 기반 알고리즘에는 feature_importance특성을 활용해 중요도를 판단할 수 있다.

반복적 특성 선택

특성을 하나도 선택하지 않은 상태로 어떠한 종료 조건에 도달 할 때까지 특성을 하나씩 추가하는 기법

모든 특성을 가지고 시작하여 어떤 종료 조건에 도달 할 때까지 특성을 하나씩 제거하는 방법 보통 사용되는 방법

일단 모델이 만들어지고 시작을 하기 때문에 일변량 분석, 모델 기반 선택보다 계산 비용이 훨씬 많이 든다.

재귀적 특성 제거(Recursive Feature Elimination) 일반적인 방법

모든 특성을 사용해서 모델을 만들고, 특성중요도가 가장 낮은 특성을 하나 제거

제거 되지 않은 특성을 모두 사용해서 또 새로운 모델을 만들어서 테스트

이 과정을 반복

특성 자동 선택 기법은 맹신하지 말자.

제곱항, 세제곱항 등을 추가하면 선형회귀 모델에 도움이 많이 됨!

log, exp, sin, cos 등의 수학 함수들을 특성에 적용하는 것도 도움이 많이 될 때가 있다.

모든 상황에서 이러한 방법이 맞는 것은 아니다. - 어떻게 보면 편법

지금은 모든 특성이 같은 속성을 가지고 있었음 ( 앞쪽에 값이 많이 몰려있는 형태 )

상황에 맞춰서 일부 특성만 변환하는 방법을 사용하기도 한다.

트리 모델에서는 이러한 변형이 불필요

선형 모델에서는 필수적으로 해야 할 수도 있다.

가끔 y를 변환해야 할 경우도 있다. y -> np.log(y+1). 카운트 같은 것들을 셀 때.

log 씌워서 훈련 시키고

exp로 결과물 복구 시키기(prediction 후)
